{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1921b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee03825-5710-4867-aa74-3bdfcca38490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: keras 3.9.2\n",
      "Uninstalling keras-3.9.2:\n",
      "  Successfully uninstalled keras-3.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall keras -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c9e3851-97bb-4bef-986c-1c7d004253f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.19.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml==6.0 in c:\\jupyter\\projects_env\\lib\\site-packages (6.0)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf-keras)\n",
      "  Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.67.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Using cached keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\jupyter\\projects_env\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\jupyter\\projects_env\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\jupyter\\projects_env\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\jupyter\\projects_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\jupyter\\projects_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\jupyter\\projects_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\jupyter\\projects_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\jupyter\\projects_env\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\jupyter\\projects_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\jupyter\\projects_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\jupyter\\projects_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.19.0-cp311-cp311-win_amd64.whl (375.9 MB)\n",
      "Using cached keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "Installing collected packages: keras, tensorflow, tf-keras\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.18.0\n",
      "    Uninstalling tensorflow-2.18.0:\n",
      "      Successfully uninstalled tensorflow-2.18.0\n",
      "Successfully installed keras-3.9.2 tensorflow-2.19.0 tf-keras-2.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tf-keras pyyaml==6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f4d5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: transformers in c:\\jupyter\\llm_env\\lib\\site-packages (4.30.0)\n",
      "Requirement already satisfied: filelock in c:\\jupyter\\llm_env\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\jupyter\\llm_env\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\jupyter\\llm_env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\jupyter\\llm_env\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\jupyter\\llm_env\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\jupyter\\llm_env\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\jupyter\\llm_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\jupyter\\llm_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\jupyter\\llm_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\jupyter\\llm_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\jupyter\\llm_env\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\jupyter\\llm_env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\jupyter\\llm_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\jupyter\\llm_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\jupyter\\llm_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\jupyter\\llm_env\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\jupyter\\llm_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\jupyter\\llm_env\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\jupyter\\llm_env\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading torch-2.6.0-cp311-cp311-win_amd64.whl (204.2 MB)\n",
      "   ---------------------------------------- 0.0/204.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/204.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/204.2 MB 1.7 MB/s eta 0:02:02\n",
      "   ---------------------------------------- 0.8/204.2 MB 1.2 MB/s eta 0:02:56\n",
      "   ---------------------------------------- 1.0/204.2 MB 1.3 MB/s eta 0:02:42\n",
      "   ---------------------------------------- 1.6/204.2 MB 1.5 MB/s eta 0:02:18\n",
      "   ---------------------------------------- 1.8/204.2 MB 1.5 MB/s eta 0:02:19\n",
      "   ---------------------------------------- 2.1/204.2 MB 1.5 MB/s eta 0:02:15\n",
      "    --------------------------------------- 2.6/204.2 MB 1.7 MB/s eta 0:02:02\n",
      "    --------------------------------------- 2.9/204.2 MB 1.7 MB/s eta 0:01:59\n",
      "    --------------------------------------- 3.1/204.2 MB 1.6 MB/s eta 0:02:08\n",
      "    --------------------------------------- 3.7/204.2 MB 1.6 MB/s eta 0:02:03\n",
      "    --------------------------------------- 4.2/204.2 MB 1.7 MB/s eta 0:01:57\n",
      "    --------------------------------------- 4.5/204.2 MB 1.7 MB/s eta 0:01:57\n",
      "    --------------------------------------- 4.7/204.2 MB 1.7 MB/s eta 0:02:00\n",
      "    --------------------------------------- 5.0/204.2 MB 1.7 MB/s eta 0:02:01\n",
      "   - -------------------------------------- 5.5/204.2 MB 1.7 MB/s eta 0:01:59\n",
      "   - -------------------------------------- 5.8/204.2 MB 1.7 MB/s eta 0:02:00\n",
      "   - -------------------------------------- 5.8/204.2 MB 1.7 MB/s eta 0:02:00\n",
      "   - -------------------------------------- 5.8/204.2 MB 1.7 MB/s eta 0:02:00\n",
      "   - -------------------------------------- 5.8/204.2 MB 1.7 MB/s eta 0:02:00\n",
      "   - -------------------------------------- 5.8/204.2 MB 1.7 MB/s eta 0:02:00\n",
      "   - -------------------------------------- 6.0/204.2 MB 1.3 MB/s eta 0:02:29\n",
      "   - -------------------------------------- 6.0/204.2 MB 1.3 MB/s eta 0:02:29\n",
      "   - -------------------------------------- 6.0/204.2 MB 1.3 MB/s eta 0:02:29\n",
      "   - -------------------------------------- 6.0/204.2 MB 1.3 MB/s eta 0:02:29\n",
      "   - -------------------------------------- 6.0/204.2 MB 1.3 MB/s eta 0:02:29\n",
      "   - -------------------------------------- 6.0/204.2 MB 1.3 MB/s eta 0:02:29\n",
      "   - -------------------------------------- 6.3/204.2 MB 1.1 MB/s eta 0:03:02\n",
      "   - -------------------------------------- 6.3/204.2 MB 1.1 MB/s eta 0:03:02\n",
      "   - -------------------------------------- 6.3/204.2 MB 1.1 MB/s eta 0:03:02\n",
      "   - -------------------------------------- 6.6/204.2 MB 1.0 MB/s eta 0:03:16\n",
      "   - -------------------------------------- 6.6/204.2 MB 1.0 MB/s eta 0:03:16\n",
      "   - -------------------------------------- 6.6/204.2 MB 1.0 MB/s eta 0:03:16\n",
      "   - -------------------------------------- 6.8/204.2 MB 970.9 kB/s eta 0:03:24\n",
      "   - -------------------------------------- 6.8/204.2 MB 970.9 kB/s eta 0:03:24\n",
      "   - -------------------------------------- 7.1/204.2 MB 960.8 kB/s eta 0:03:26\n",
      "   - -------------------------------------- 7.3/204.2 MB 961.7 kB/s eta 0:03:25\n",
      "   - -------------------------------------- 7.6/204.2 MB 972.6 kB/s eta 0:03:23\n",
      "   - -------------------------------------- 8.1/204.2 MB 1.0 MB/s eta 0:03:16\n",
      "   - -------------------------------------- 8.7/204.2 MB 1.0 MB/s eta 0:03:09\n",
      "   - -------------------------------------- 9.2/204.2 MB 1.1 MB/s eta 0:03:01\n",
      "   - -------------------------------------- 9.7/204.2 MB 1.1 MB/s eta 0:02:56\n",
      "   - -------------------------------------- 10.0/204.2 MB 1.1 MB/s eta 0:02:53\n",
      "   -- ------------------------------------- 10.5/204.2 MB 1.2 MB/s eta 0:02:48\n",
      "   -- ------------------------------------- 11.0/204.2 MB 1.2 MB/s eta 0:02:44\n",
      "   -- ------------------------------------- 11.3/204.2 MB 1.2 MB/s eta 0:02:42\n",
      "   -- ------------------------------------- 11.8/204.2 MB 1.2 MB/s eta 0:02:38\n",
      "   -- ------------------------------------- 12.3/204.2 MB 1.2 MB/s eta 0:02:34\n",
      "   -- ------------------------------------- 12.8/204.2 MB 1.3 MB/s eta 0:02:32\n",
      "   -- ------------------------------------- 13.1/204.2 MB 1.3 MB/s eta 0:02:30\n",
      "   -- ------------------------------------- 13.6/204.2 MB 1.3 MB/s eta 0:02:28\n",
      "   -- ------------------------------------- 14.2/204.2 MB 1.3 MB/s eta 0:02:25\n",
      "   -- ------------------------------------- 14.7/204.2 MB 1.3 MB/s eta 0:02:22\n",
      "   -- ------------------------------------- 15.2/204.2 MB 1.4 MB/s eta 0:02:19\n",
      "   --- ------------------------------------ 15.5/204.2 MB 1.4 MB/s eta 0:02:18\n",
      "   --- ------------------------------------ 16.0/204.2 MB 1.4 MB/s eta 0:02:16\n",
      "   --- ------------------------------------ 16.5/204.2 MB 1.4 MB/s eta 0:02:14\n",
      "   --- ------------------------------------ 16.8/204.2 MB 1.4 MB/s eta 0:02:14\n",
      "   --- ------------------------------------ 17.0/204.2 MB 1.4 MB/s eta 0:02:14\n",
      "   --- ------------------------------------ 17.3/204.2 MB 1.4 MB/s eta 0:02:13\n",
      "   --- ------------------------------------ 17.6/204.2 MB 1.4 MB/s eta 0:02:14\n",
      "   --- ------------------------------------ 17.6/204.2 MB 1.4 MB/s eta 0:02:14\n",
      "   --- ------------------------------------ 17.8/204.2 MB 1.4 MB/s eta 0:02:17\n",
      "   --- ------------------------------------ 18.1/204.2 MB 1.4 MB/s eta 0:02:16\n",
      "   --- ------------------------------------ 18.4/204.2 MB 1.4 MB/s eta 0:02:16\n",
      "   --- ------------------------------------ 19.1/204.2 MB 1.4 MB/s eta 0:02:12\n",
      "   --- ------------------------------------ 19.7/204.2 MB 1.4 MB/s eta 0:02:10\n",
      "   --- ------------------------------------ 20.2/204.2 MB 1.4 MB/s eta 0:02:08\n",
      "   ---- ----------------------------------- 20.7/204.2 MB 1.5 MB/s eta 0:02:07\n",
      "   ---- ----------------------------------- 21.2/204.2 MB 1.5 MB/s eta 0:02:05\n",
      "   ---- ----------------------------------- 21.5/204.2 MB 1.5 MB/s eta 0:02:05\n",
      "   ---- ----------------------------------- 21.8/204.2 MB 1.5 MB/s eta 0:02:04\n",
      "   ---- ----------------------------------- 22.3/204.2 MB 1.5 MB/s eta 0:02:03\n",
      "   ---- ----------------------------------- 22.8/204.2 MB 1.5 MB/s eta 0:02:02\n",
      "   ---- ----------------------------------- 23.3/204.2 MB 1.5 MB/s eta 0:02:00\n",
      "   ---- ----------------------------------- 23.6/204.2 MB 1.5 MB/s eta 0:02:00\n",
      "   ---- ----------------------------------- 23.9/204.2 MB 1.5 MB/s eta 0:01:59\n",
      "   ---- ----------------------------------- 24.1/204.2 MB 1.5 MB/s eta 0:01:59\n",
      "   ---- ----------------------------------- 24.6/204.2 MB 1.5 MB/s eta 0:01:59\n",
      "   ---- ----------------------------------- 25.2/204.2 MB 1.5 MB/s eta 0:01:58\n",
      "   ---- ----------------------------------- 25.4/204.2 MB 1.5 MB/s eta 0:01:57\n",
      "   ----- ---------------------------------- 25.7/204.2 MB 1.5 MB/s eta 0:01:58\n",
      "   ----- ---------------------------------- 25.7/204.2 MB 1.5 MB/s eta 0:01:58\n",
      "   ----- ---------------------------------- 25.7/204.2 MB 1.5 MB/s eta 0:01:58\n",
      "   ----- ---------------------------------- 26.2/204.2 MB 1.5 MB/s eta 0:01:59\n",
      "   ----- ---------------------------------- 26.5/204.2 MB 1.5 MB/s eta 0:01:59\n",
      "   ----- ---------------------------------- 26.5/204.2 MB 1.5 MB/s eta 0:01:59\n",
      "   ----- ---------------------------------- 26.7/204.2 MB 1.5 MB/s eta 0:02:00\n",
      "   ----- ---------------------------------- 26.7/204.2 MB 1.5 MB/s eta 0:02:00\n",
      "   ----- ---------------------------------- 27.0/204.2 MB 1.5 MB/s eta 0:02:01\n",
      "   ----- ---------------------------------- 27.5/204.2 MB 1.5 MB/s eta 0:02:01\n",
      "   ----- ---------------------------------- 28.0/204.2 MB 1.5 MB/s eta 0:02:00\n",
      "   ----- ---------------------------------- 28.3/204.2 MB 1.5 MB/s eta 0:01:59\n",
      "   ----- ---------------------------------- 28.8/204.2 MB 1.5 MB/s eta 0:01:58\n",
      "   ----- ---------------------------------- 29.1/204.2 MB 1.5 MB/s eta 0:01:57\n",
      "   ----- ---------------------------------- 29.6/204.2 MB 1.5 MB/s eta 0:01:57\n",
      "   ----- ---------------------------------- 29.9/204.2 MB 1.5 MB/s eta 0:01:56\n",
      "   ----- ---------------------------------- 30.1/204.2 MB 1.5 MB/s eta 0:01:56\n",
      "   ------ --------------------------------- 30.7/204.2 MB 1.5 MB/s eta 0:01:56\n",
      "   ------ --------------------------------- 31.2/204.2 MB 1.5 MB/s eta 0:01:54\n",
      "   ------ --------------------------------- 31.7/204.2 MB 1.5 MB/s eta 0:01:54\n",
      "   ------ --------------------------------- 32.0/204.2 MB 1.5 MB/s eta 0:01:53\n",
      "   ------ --------------------------------- 32.5/204.2 MB 1.5 MB/s eta 0:01:52\n",
      "   ------ --------------------------------- 32.8/204.2 MB 1.5 MB/s eta 0:01:52\n",
      "   ------ --------------------------------- 33.0/204.2 MB 1.5 MB/s eta 0:01:52\n",
      "   ------ --------------------------------- 33.3/204.2 MB 1.5 MB/s eta 0:01:52\n",
      "   ------ --------------------------------- 33.8/204.2 MB 1.5 MB/s eta 0:01:51\n",
      "   ------ --------------------------------- 34.1/204.2 MB 1.5 MB/s eta 0:01:51\n",
      "   ------ --------------------------------- 34.3/204.2 MB 1.5 MB/s eta 0:01:51\n",
      "   ------ --------------------------------- 34.6/204.2 MB 1.5 MB/s eta 0:01:51\n",
      "   ------ --------------------------------- 35.1/204.2 MB 1.5 MB/s eta 0:01:51\n",
      "   ------ --------------------------------- 35.4/204.2 MB 1.5 MB/s eta 0:01:50\n",
      "   ------- -------------------------------- 35.9/204.2 MB 1.5 MB/s eta 0:01:50\n",
      "   ------- -------------------------------- 36.2/204.2 MB 1.5 MB/s eta 0:01:50\n",
      "   ------- -------------------------------- 36.4/204.2 MB 1.5 MB/s eta 0:01:50\n",
      "   ------- -------------------------------- 36.7/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.0/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.2/204.2 MB 1.5 MB/s eta 0:01:49\n",
      "   ------- -------------------------------- 37.5/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 37.7/204.2 MB 1.4 MB/s eta 0:02:02\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.3/204.2 MB 1.1 MB/s eta 0:02:26\n",
      "   ------- -------------------------------- 38.5/204.2 MB 1.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 38.5/204.2 MB 1.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 38.5/204.2 MB 1.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 38.5/204.2 MB 1.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 38.5/204.2 MB 1.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 38.5/204.2 MB 1.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 38.5/204.2 MB 1.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 38.5/204.2 MB 1.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 38.5/204.2 MB 1.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 38.5/204.2 MB 1.1 MB/s eta 0:02:31\n",
      "   ------- -------------------------------- 38.8/204.2 MB 1.1 MB/s eta 0:02:35\n",
      "   ------- -------------------------------- 38.8/204.2 MB 1.1 MB/s eta 0:02:35\n",
      "   ------- -------------------------------- 38.8/204.2 MB 1.1 MB/s eta 0:02:35\n",
      "   ------- -------------------------------- 38.8/204.2 MB 1.1 MB/s eta 0:02:35\n",
      "   ------- -------------------------------- 38.8/204.2 MB 1.1 MB/s eta 0:02:35\n",
      "   ------- -------------------------------- 38.8/204.2 MB 1.1 MB/s eta 0:02:35\n",
      "   ------- -------------------------------- 38.8/204.2 MB 1.1 MB/s eta 0:02:35\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n",
      "   ------- ------------------------------- 39.1/204.2 MB 996.6 kB/s eta 0:02:46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bodas Lokesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py\", line 465, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bodas Lokesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bodas Lokesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py\", line 1278, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bodas Lokesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py\", line 1134, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 106, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 97, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 386, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 554, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 469, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 184, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 55, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "  File \"C:\\Users\\Bodas Lokesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py\", line 155, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"C:\\jupyter\\llm_env\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa057d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Positive</th>\n",
       "      <th>im getting on borderlands and i will murder you all ,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2401  Borderlands  Positive  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "...     ...          ...       ...   \n",
       "74676  9200       Nvidia  Positive   \n",
       "74677  9200       Nvidia  Positive   \n",
       "74678  9200       Nvidia  Positive   \n",
       "74679  9200       Nvidia  Positive   \n",
       "74680  9200       Nvidia  Positive   \n",
       "\n",
       "      im getting on borderlands and i will murder you all ,  \n",
       "0      I am coming to the borders and I will kill you...     \n",
       "1      im getting on borderlands and i will kill you ...     \n",
       "2      im coming on borderlands and i will murder you...     \n",
       "3      im getting on borderlands 2 and i will murder ...     \n",
       "4      im getting into borderlands and i can murder y...     \n",
       "...                                                  ...     \n",
       "74676  Just realized that the Windows partition of my...     \n",
       "74677  Just realized that my Mac window partition is ...     \n",
       "74678  Just realized the windows partition of my Mac ...     \n",
       "74679  Just realized between the windows partition of...     \n",
       "74680  Just like the windows partition of my Mac is l...     \n",
       "\n",
       "[74681 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"twitter_training.csv\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ba6142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2401                                                       0\n",
       "Borderlands                                                0\n",
       "Positive                                                   0\n",
       "im getting on borderlands and i will murder you all ,    686\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ba6baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>im getting on borderlands and i will murder you all ,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Positive im getting on borderlands and i will murder you all ,\n",
       "0      Positive  I am coming to the borders and I will kill you...   \n",
       "1      Positive  im getting on borderlands and i will kill you ...   \n",
       "2      Positive  im coming on borderlands and i will murder you...   \n",
       "3      Positive  im getting on borderlands 2 and i will murder ...   \n",
       "4      Positive  im getting into borderlands and i can murder y...   \n",
       "...         ...                                                ...   \n",
       "74676  Positive  Just realized that the Windows partition of my...   \n",
       "74677  Positive  Just realized that my Mac window partition is ...   \n",
       "74678  Positive  Just realized the windows partition of my Mac ...   \n",
       "74679  Positive  Just realized between the windows partition of...   \n",
       "74680  Positive  Just like the windows partition of my Mac is l...   \n",
       "\n",
       "[74681 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.drop(['2401', 'Borderlands'], axis = 1)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7358e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Sentiments', 'Comments']\n",
    "train_data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012ed72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiments</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiments                                           Comments\n",
       "0       Positive  I am coming to the borders and I will kill you...\n",
       "1       Positive  im getting on borderlands and i will kill you ...\n",
       "2       Positive  im coming on borderlands and i will murder you...\n",
       "3       Positive  im getting on borderlands 2 and i will murder ...\n",
       "4       Positive  im getting into borderlands and i can murder y...\n",
       "...          ...                                                ...\n",
       "74676   Positive  Just realized that the Windows partition of my...\n",
       "74677   Positive  Just realized that my Mac window partition is ...\n",
       "74678   Positive  Just realized the windows partition of my Mac ...\n",
       "74679   Positive  Just realized between the windows partition of...\n",
       "74680   Positive  Just like the windows partition of my Mac is l...\n",
       "\n",
       "[74681 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd6310c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiments\n",
       "Negative      22542\n",
       "Positive      20831\n",
       "Neutral       18318\n",
       "Irrelevant    12990\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Sentiments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9c9544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3364</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>Irrelevant</th>\n",
       "      <th>I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>352</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4371</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4433</td>\n",
       "      <td>Google</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6273</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hi @EAHelp I’ve had Madeleine McCann in my cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>4891</td>\n",
       "      <td>GrandTheftAuto(GTA)</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4359</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2652</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>8069</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>6960</td>\n",
       "      <td>johnson&amp;johnson</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     3364             Facebook  Irrelevant  \\\n",
       "0     352               Amazon     Neutral   \n",
       "1    8312            Microsoft    Negative   \n",
       "2    4371                CS-GO    Negative   \n",
       "3    4433               Google     Neutral   \n",
       "4    6273                 FIFA    Negative   \n",
       "..    ...                  ...         ...   \n",
       "994  4891  GrandTheftAuto(GTA)  Irrelevant   \n",
       "995  4359                CS-GO  Irrelevant   \n",
       "996  2652          Borderlands    Positive   \n",
       "997  8069            Microsoft    Positive   \n",
       "998  6960      johnson&johnson     Neutral   \n",
       "\n",
       "    I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣  \n",
       "0    BBC News - Amazon boss Jeff Bezos rejects clai...                                                                                                                                                                                                  \n",
       "1    @Microsoft Why do I pay for WORD when it funct...                                                                                                                                                                                                  \n",
       "2    CSGO matchmaking is so full of closet hacking,...                                                                                                                                                                                                  \n",
       "3    Now the President is slapping Americans in the...                                                                                                                                                                                                  \n",
       "4    Hi @EAHelp I’ve had Madeleine McCann in my cel...                                                                                                                                                                                                  \n",
       "..                                                 ...                                                                                                                                                                                                  \n",
       "994  ⭐️ Toronto is the arts and culture capital of ...                                                                                                                                                                                                  \n",
       "995  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...                                                                                                                                                                                                  \n",
       "996  Today sucked so it’s time to drink wine n play...                                                                                                                                                                                                  \n",
       "997  Bought a fraction of Microsoft today. Small wins.                                                                                                                                                                                                  \n",
       "998  Johnson & Johnson to stop selling talc baby po...                                                                                                                                                                                                  \n",
       "\n",
       "[999 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = pd.read_csv(\"twitter_validation.csv\")\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39c2afeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Irrelevant</th>\n",
       "      <th>I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Hi @EAHelp I’ve had Madeleine McCann in my cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Irrelevant  \\\n",
       "0       Neutral   \n",
       "1      Negative   \n",
       "2      Negative   \n",
       "3       Neutral   \n",
       "4      Negative   \n",
       "..          ...   \n",
       "994  Irrelevant   \n",
       "995  Irrelevant   \n",
       "996    Positive   \n",
       "997    Positive   \n",
       "998     Neutral   \n",
       "\n",
       "    I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom’s great auntie as ‘Hayley can’t get out of bed’ and told to his grandma, who now thinks I’m a lazy, terrible person 🤣  \n",
       "0    BBC News - Amazon boss Jeff Bezos rejects clai...                                                                                                                                                                                                  \n",
       "1    @Microsoft Why do I pay for WORD when it funct...                                                                                                                                                                                                  \n",
       "2    CSGO matchmaking is so full of closet hacking,...                                                                                                                                                                                                  \n",
       "3    Now the President is slapping Americans in the...                                                                                                                                                                                                  \n",
       "4    Hi @EAHelp I’ve had Madeleine McCann in my cel...                                                                                                                                                                                                  \n",
       "..                                                 ...                                                                                                                                                                                                  \n",
       "994  ⭐️ Toronto is the arts and culture capital of ...                                                                                                                                                                                                  \n",
       "995  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...                                                                                                                                                                                                  \n",
       "996  Today sucked so it’s time to drink wine n play...                                                                                                                                                                                                  \n",
       "997  Bought a fraction of Microsoft today. Small wins.                                                                                                                                                                                                  \n",
       "998  Johnson & Johnson to stop selling talc baby po...                                                                                                                                                                                                  \n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data = validation_data.drop(['3364', 'Facebook'], axis = 1)\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7152ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Sentiments', 'Comments']\n",
    "validation_data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65026f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiments</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Hi @EAHelp I’ve had Madeleine McCann in my cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiments                                           Comments\n",
       "0       Neutral  BBC News - Amazon boss Jeff Bezos rejects clai...\n",
       "1      Negative  @Microsoft Why do I pay for WORD when it funct...\n",
       "2      Negative  CSGO matchmaking is so full of closet hacking,...\n",
       "3       Neutral  Now the President is slapping Americans in the...\n",
       "4      Negative  Hi @EAHelp I’ve had Madeleine McCann in my cel...\n",
       "..          ...                                                ...\n",
       "994  Irrelevant  ⭐️ Toronto is the arts and culture capital of ...\n",
       "995  Irrelevant  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...\n",
       "996    Positive  Today sucked so it’s time to drink wine n play...\n",
       "997    Positive  Bought a fraction of Microsoft today. Small wins.\n",
       "998     Neutral  Johnson & Johnson to stop selling talc baby po...\n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8fb3395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiments\n",
       "Neutral       285\n",
       "Positive      277\n",
       "Negative      266\n",
       "Irrelevant    171\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data['Sentiments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01b9f261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiments</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74676</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74681 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiments                                           Comments\n",
       "0       Positive  I am coming to the borders and I will kill you...\n",
       "1       Positive  im getting on borderlands and i will kill you ...\n",
       "2       Positive  im coming on borderlands and i will murder you...\n",
       "3       Positive  im getting on borderlands 2 and i will murder ...\n",
       "4       Positive  im getting into borderlands and i can murder y...\n",
       "...          ...                                                ...\n",
       "74676   Positive  Just realized that the Windows partition of my...\n",
       "74677   Positive  Just realized that my Mac window partition is ...\n",
       "74678   Positive  Just realized the windows partition of my Mac ...\n",
       "74679   Positive  Just realized between the windows partition of...\n",
       "74680   Positive  Just like the windows partition of my Mac is l...\n",
       "\n",
       "[74681 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Sentiments'] = train_data['Sentiments'].replace(\"Irrelevant\",'Neutral')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b5d23de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiments\n",
       "Neutral     31308\n",
       "Negative    22542\n",
       "Positive    20831\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Sentiments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e534a253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiments</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Hi @EAHelp I’ve had Madeleine McCann in my cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiments                                           Comments\n",
       "0      Neutral  BBC News - Amazon boss Jeff Bezos rejects clai...\n",
       "1     Negative  @Microsoft Why do I pay for WORD when it funct...\n",
       "2     Negative  CSGO matchmaking is so full of closet hacking,...\n",
       "3      Neutral  Now the President is slapping Americans in the...\n",
       "4     Negative  Hi @EAHelp I’ve had Madeleine McCann in my cel...\n",
       "..         ...                                                ...\n",
       "994    Neutral  ⭐️ Toronto is the arts and culture capital of ...\n",
       "995    Neutral  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...\n",
       "996   Positive  Today sucked so it’s time to drink wine n play...\n",
       "997   Positive  Bought a fraction of Microsoft today. Small wins.\n",
       "998    Neutral  Johnson & Johnson to stop selling talc baby po...\n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data['Sentiments'] = validation_data['Sentiments'].replace(\"Irrelevant\",'Neutral')\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43fb8d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiments\n",
       "Neutral     456\n",
       "Positive    277\n",
       "Negative    266\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data['Sentiments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "223af1ed-e690-45d5-a0d7-8435ae4752a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train labels: 0\n",
      "Missing values in validation labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN in labels\n",
    "print(\"Missing values in train labels:\", train_data['Sentiments'].isna().sum())\n",
    "print(\"Missing values in validation labels:\", validation_data['Sentiments'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee61169c-263e-4e4e-a527-5bcbdbbac56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique train labels: ['Positive' 'Neutral' 'Negative']\n",
      "Unique val labels: ['Neutral' 'Negative' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing labels\n",
    "train_data = train_data.dropna(subset=['Sentiments'])\n",
    "validation_data = validation_data.dropna(subset=['Sentiments'])\n",
    "\n",
    "# Verify unique labels\n",
    "print(\"Unique train labels:\", train_data['Sentiments'].unique())\n",
    "print(\"Unique val labels:\", validation_data['Sentiments'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80d701ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sentiments                                           Comments\n",
      "0       Positive  I am coming to the borders and I will kill you...\n",
      "1       Positive  im getting on borderlands and i will kill you all\n",
      "2       Positive  im coming on borderlands and i will murder you...\n",
      "3       Positive  im getting on borderlands 2 and i will murder ...\n",
      "4       Positive  im getting into borderlands and i can murder y...\n",
      "...          ...                                                ...\n",
      "74676   Positive  Just realized that the Windows partition of my...\n",
      "74677   Positive  Just realized that my Mac window partition is ...\n",
      "74678   Positive  Just realized the windows partition of my Mac ...\n",
      "74679   Positive  Just realized between the windows partition of...\n",
      "74680   Positive  Just like the windows partition of my Mac is l...\n",
      "\n",
      "[74681 rows x 2 columns]\n",
      "    Sentiments                                           Comments\n",
      "0      Neutral  BBC News Amazon boss Jeff Bezos rejects claims...\n",
      "1     Negative  USER Why do I pay for WORD when it functions s...\n",
      "2     Negative  CSGO matchmaking is so full of closet hacking ...\n",
      "3      Neutral  Now the President is slapping Americans in the...\n",
      "4     Negative  Hi USER I ve had Madeleine McCann in my cellar...\n",
      "..         ...                                                ...\n",
      "994    Neutral  Toronto is the arts and culture capital of Can...\n",
      "995    Neutral  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...\n",
      "996   Positive  Today sucked so it s time to drink wine n play...\n",
      "997   Positive    Bought a fraction of Microsoft today Small wins\n",
      "998    Neutral  Johnson Johnson to stop selling talc baby powd...\n",
      "\n",
      "[999 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def transformer_preprocessing(tweet):\n",
    "\n",
    "    # Convert to string in case of non-string inputs\n",
    "    text = str(tweet)\n",
    "    # 1. Remove URLs but keep mentions and hashtags\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # 2. Replace user mentions with [USER] (optional)\n",
    "    text = re.sub(r'@\\w+', '[USER]', text)\n",
    "    # 3. Remove special characters except those important for sentiment\n",
    "    text = re.sub(r\"[^a-zA-Z0-9#!?]+\", ' ', text)\n",
    "    # 4. Fix whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    #return tweet.strip()\n",
    "    return text\n",
    "\n",
    "train_data['Comments'] = train_data['Comments'].apply(transformer_preprocessing)\n",
    "validation_data['Comments'] = validation_data['Comments'].apply(transformer_preprocessing)\n",
    "\n",
    "print(train_data)\n",
    "print(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d62e189c-0905-4977-980b-d2f04511a112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 2.385364762999872, 1: 3.3129713423831073, 2: 3.585089530027363}\n"
     ]
    }
   ],
   "source": [
    "class_counts = train_data['Sentiments'].value_counts()\n",
    "class_weights = {i: sum(class_counts)/count for i, count in enumerate(class_counts)}\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9aacd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit only on training data\n",
    "train_data['Sentiments'] = le.fit_transform(train_data['Sentiments'])\n",
    "# Transform validation data with same encoder\n",
    "validation_data['Sentiments'] = le.transform(validation_data['Sentiments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "265a2f6b-6461-4abb-816c-9ed9f7732d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distribution:\n",
      "Sentiments\n",
      "1    31308\n",
      "0    22542\n",
      "2    20831\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train distribution:\")\n",
    "print(train_data['Sentiments'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6611a79b-5b69-4cd0-903a-f33262b63093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation distribution:\n",
      "Sentiments\n",
      "1    456\n",
      "2    277\n",
      "0    266\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValidation distribution:\")\n",
    "print(validation_data['Sentiments'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53aebb-119c-439e-910e-a5f8e2c6e2d3",
   "metadata": {},
   "source": [
    "<h2>Adjust Class Weights Calculation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "836bf384-5852-4666-9a04-8380facd6a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted class weights: {0: 3.3129713423831073, 1: 2.385364762999872, 2: 3.585089530027363}\n"
     ]
    }
   ],
   "source": [
    "# Calculate inverse frequency weights\n",
    "class_counts = train_data['Sentiments'].value_counts()\n",
    "\n",
    "class_weights = {\n",
    "    0: sum(class_counts)/class_counts[0],\n",
    "    1: sum(class_counts)/class_counts[1],\n",
    "    2: sum(class_counts)/class_counts[2]\n",
    "}\n",
    "print(\"Adjusted class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2247215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\jupyter\\llm_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Transformer Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_data(texts, max_length=128):\n",
    "    \n",
    "    # Tokenize with verification\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "    # Verify output shapes\n",
    "    assert encodings['input_ids'].shape[0] == len(texts), \"Mismatch in tokenized samples\"\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fd0c583-5bfb-4625-84f7-31d7ab5026f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\jupyter\\llm_env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenize_data(train_data['Comments'].tolist())\n",
    "val_encodings = tokenize_data(validation_data['Comments'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad84f79-f30e-429c-a6c8-263836f4a1de",
   "metadata": {},
   "source": [
    "<h2>Dataset Preparation with Class Weights</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ada0a30-8e23-40ee-bb24-62aa39f016d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "label_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "y_train = train_data['Sentiments'].map(label_map).fillna(-1).astype(int)\n",
    "y_val = validation_data['Sentiments'].map(label_map).fillna(-1).astype(int)\n",
    "\n",
    "# Create weighted datasets\n",
    "def create_weighted_dataset(encodings, labels, class_weights, batch_size=16):\n",
    "    # Convert to numpy arrays\n",
    "    labels = labels.values\n",
    "    \n",
    "    # Create weights array\n",
    "    weights = np.array([class_weights[x] for x in labels])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(encodings),\n",
    "        tf.keras.utils.to_categorical(labels, num_classes=3),\n",
    "        weights\n",
    "    ))\n",
    "    \n",
    "    return dataset.shuffle(1000).batch(batch_size).prefetch(2)\n",
    "\n",
    "train_dataset = create_weighted_dataset(train_encodings, train_data['Sentiments'], class_weights)\n",
    "val_dataset = create_weighted_dataset(val_encodings, validation_data['Sentiments'], class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c5e3f-bbf9-4cf3-86f4-08c2125af21a",
   "metadata": {},
   "source": [
    "<h2>Updated Model Architecture(Model Training with Weighted Tokenization)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76397ff7-e7cf-46a6-96ec-8b6c0385a082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\jupyter\\llm_env\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66955779 (255.42 MB)\n",
      "Trainable params: 66955779 (255.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=3,\n",
    "    id2label={i: label for i, label in enumerate(le.classes_)},\n",
    "    dropout=0.3,\n",
    "    attention_dropout=0.3\n",
    ")\n",
    "\n",
    "# Custom optimizer with weight decay\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    "    weighted_metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Verify model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85ea58fd-d726-4b5b-b226-a5e220eb18e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\jupyter\\llm_env\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\jupyter\\llm_env\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4668/4668 [==============================] - 50850s 11s/step - loss: 1.9745 - accuracy: 0.7105 - weighted_accuracy: 0.7208 - val_loss: 1.7602 - val_accuracy: 0.7487 - val_weighted_accuracy: 0.7654\n",
      "Epoch 2/10\n",
      "4668/4668 [==============================] - 26641s 6s/step - loss: 1.3063 - accuracy: 0.8205 - weighted_accuracy: 0.8259 - val_loss: 1.4580 - val_accuracy: 0.8358 - val_weighted_accuracy: 0.8253\n",
      "Epoch 3/10\n",
      "4668/4668 [==============================] - 7275s 2s/step - loss: 0.8585 - accuracy: 0.8848 - weighted_accuracy: 0.8874 - val_loss: 1.1400 - val_accuracy: 0.8699 - val_weighted_accuracy: 0.8789\n",
      "Epoch 4/10\n",
      "  18/4668 [..............................] - ETA: 1:58:44 - loss: 1.3396 - accuracy: 0.8438 - weighted_accuracy: 0.8566"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train with class weights\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1799\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.profiler.experimental.Trace(\n\u001b[32m   1800\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1801\u001b[39m     epoch_num=epoch,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1804\u001b[39m     _r=\u001b[32m1\u001b[39m,\n\u001b[32m   1805\u001b[39m ):\n\u001b[32m   1806\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m-> \u001b[39m\u001b[32m1807\u001b[39m     tmp_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_handler.should_sync:\n\u001b[32m   1809\u001b[39m         context.async_wait()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    829\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    835\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    865\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    866\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    867\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    872\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    873\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    874\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1319\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1321\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1322\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1324\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1325\u001b[39m     args,\n\u001b[32m   1326\u001b[39m     possible_gradient_type,\n\u001b[32m   1327\u001b[39m     executing_eagerly)\n\u001b[32m   1328\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1484\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1494\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1495\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1496\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1500\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1501\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\jupyter\\llm_env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train with class weights\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    \n",
    "    epochs=5,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c01fec9-0a3a-40de-9e52-5f5a3b2bfc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 25s 403ms/step - loss: 0.9806 - accuracy: 0.8699 - weighted_accuracy: 0.8836\n",
      "\n",
      "Validation Loss: 0.9806\n",
      "Standard Accuracy: 86.99%\n",
      "Weighted Accuracy: 88.36%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model and get all metrics\n",
    "results = model.evaluate(val_dataset)\n",
    "val_loss = results[0]\n",
    "val_accuracy = results[1]  # Standard accuracy\n",
    "val_weighted_accuracy = results[2]  # Weighted accuracy\n",
    "\n",
    "print(f\"\\nValidation Loss: {val_loss:.4f}\")\n",
    "print(f\"Standard Accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Weighted Accuracy: {val_weighted_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc645040-3323-4499-a79f-8ab3e7582fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 26s 412ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m y_pred_classes = tf.argmax(y_pred.logits, axis=\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Convert one-hot encoded labels back to classes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m y_true = tf.argmax(\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43munbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      3\u001b[39m y_pred_classes = tf.argmax(y_pred.logits, axis=\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Convert one-hot encoded labels back to classes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m y_true = tf.argmax([label \u001b[38;5;28;01mfor\u001b[39;00m _, label \u001b[38;5;129;01min\u001b[39;00m val_dataset.unbatch()], axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "y_pred = model.predict(val_dataset)\n",
    "y_pred_classes = tf.argmax(y_pred.logits, axis=1).numpy()\n",
    "\n",
    "# Extract true labels correctly (dataset contains 3 elements: inputs, labels, weights)\n",
    "y_true = tf.argmax(\n",
    "    [label for _, label, _ in val_dataset.unbatch()],  # Add third element unpack\n",
    "    axis=1\n",
    ").numpy()\n",
    "\n",
    "# Alternative: Directly use encoded labels from DataFrame (no shuffling in validation)\n",
    "# y_true = validation_data['Sentiments'].values  # Only if dataset wasn't shuffled\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3708fe-b62c-45cd-be17-59c1148c1613",
   "metadata": {},
   "source": [
    "<h2>Classification Report</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d803d002-f36f-41fa-a76e-94b44946971c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mClassification Report:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(\u001b[43my_true\u001b[49m, y_pred_classes, target_names=label_map.keys()))\n",
      "\u001b[31mNameError\u001b[39m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6626b51-5ccc-4940-95a8-493b0e223d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_map.keys(),\n",
    "            yticklabels=label_map.keys())\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e67a4-9570-4b03-b966-8fdd2e6ee5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-wise Accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "for class_name, acc in zip(label_map.keys(), class_accuracy):\n",
    "    print(f\"{class_name} Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c3e6a-9702-4dea-8dbe-354280f63c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
